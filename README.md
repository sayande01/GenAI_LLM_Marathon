# üöÄ 30-Day GenAI & LLM Marathon

Welcome to my **30-Day GenAI & LLM Marathon**! üåü In this repository, I document my journey into the fascinating world of Generative AI and Large Language Models (LLMs). Over the next 30 days, I‚Äôll explore key topics, implement hands-on projects, and share detailed notebooks that cover everything from basic concepts to advanced applications‚Äîall using Python, PyTorch/TensorFlow, and cutting-edge libraries like Hugging Face.

---

## üìÖ Marathon Overview

This 30-day challenge is designed to provide a deep, practical understanding of GenAI and LLMs through daily milestones. Some of the key topics include:

- **ü§ñ Generative AI & LLMs:** An overview of AI that can generate text, code, images, and more.
- **üß© Transformer Architecture:** Explore the self-attention mechanism and encoder-decoder structures.
- **üöÄ Hugging Face Basics:** Learn to work with pre-trained models, tokenizers, and pipelines.
- **‚úçÔ∏è Prompt Engineering:** Master crafting effective prompts for various AI tasks.
- **üéØ Fine-Tuning & Evaluation:** Adapt models to specific tasks and assess performance using metrics.
- **üîó LangChain & RAG:** Build AI applications and pipelines that combine retrieval and generation.
- **üõ°Ô∏è Ethical AI & Security:** Address bias, fairness, and secure deployment practices.
- **üåê Deployment:** Learn to deploy your models and create APIs with FastAPI/Flask.

Each day includes:
- **üìñ Concept Overview:** Clear explanations of core topics.
- **üî¢ Math & Theory:** Key equations and underlying principles.
- **üíª Hands-On Tasks:** Practical coding in Jupyter Notebooks.
- **üìù Outputs:** Well-documented notebooks and READMEs summarizing learnings.

---

## üõ†Ô∏è Technologies & Tools

Throughout this marathon, I will be using:
- **Programming Language:** Python üêç
- **Deep Learning & NLP Frameworks:** TensorFlow, PyTorch, Hugging Face Transformers
- **Visualization:** Matplotlib, Seaborn
- **Data Processing:** NumPy, Pandas
- **Notebook Environment:** Jupyter Notebook / Google Colab
- **Deployment Tools:** FastAPI, Flask, Docker

---

## üìú Day-by-Day Breakdown

Below is a snapshot of the topics covered on select days. (Click the notebook links to dive into the details for each day.)

| Day & Date            | Main Topic                                         | Highlights & Outputs |
| --------------------- | -------------------------------------------------- | -------------------- |
| **Day 1 (Feb 3)**     | **Introduction to Generative AI & LLMs**           | Overview, applications, key players (OpenAI, Hugging Face) <br> [Notebook: Day_1/Intro_to_Generative_AI.ipynb](Day_1/Intro_to_Generative_AI.ipynb) |
| **Day 2 (Feb 4)**     | **Transformer Architecture**                       | Self-attention, encoder-decoder, positional encoding <br> [Notebook: Day_2/Transformer_Implementation.ipynb](Day_2/Transformer_Implementation.ipynb) |
| **Day 3 (Feb 5)**     | **Hugging Face Basics**                            | Pre-trained models (GPT-2, BERT), tokenizers, pipelines <br> [Notebook: Day_3/HuggingFace_Basics.ipynb](Day_3/HuggingFace_Basics.ipynb) |
| **Day 4 (Feb 6)**     | **Prompt Engineering Basics**                      | Crafting effective prompts, zero-shot & few-shot learning <br> [Notebook: Day_4/Prompt_Engineering.ipynb](Day_4/Prompt_Engineering.ipynb) |
| **Day 5 (Feb 7)**     | **Fine-Tuning LLMs**                               | Fine-tuning pre-trained models on custom datasets <br> [Notebook: Day_5/Fine_Tuning_LLM.ipynb](Day_5/Fine_Tuning_LLM.ipynb) |
| **Day 6 (Feb 8)**     | **Evaluation Metrics for LLMs**                    | BLEU, ROUGE, perplexity, comparing models <br> [Notebook: Day_6/Evaluation_Metrics.ipynb](Day_6/Evaluation_Metrics.ipynb) |
| **Day 7 (Feb 9)**     | **Introduction to LangChain**                      | Building LLM-powered apps, modular workflows <br> [Notebook: Day_7/LangChain_Chatbot.ipynb](Day_7/LangChain_Chatbot.ipynb) |
| **Day 8 (Feb 10)**    | **Vector Databases**                               | Embedding storage & retrieval with tools like Pinecone <br> [Notebook: Day_8/Vector_Database.ipynb](Day_8/Vector_Database.ipynb) |
| **Day 9 (Feb 11)**    | **Retrieval-Augmented Generation (RAG)**           | Combining retrieval and generation for Q&A tasks <br> [Notebook: Day_9/RAG_Pipeline.ipynb](Day_9/RAG_Pipeline.ipynb) |
| **Day 10 (Feb 12)**   | **Advanced Prompt Engineering**                    | Advanced techniques: chain-of-thought, prompt chaining <br> [Notebook: Day_10/Advanced_Prompt_Engineering.ipynb](Day_10/Advanced_Prompt_Engineering.ipynb) |
| **Day 11 (Feb 13)**   | **Ethical AI in Generative Models**                | Bias, fairness, and mitigation strategies <br> [Notebook: Day_11/Ethical_AI.ipynb](Day_11/Ethical_AI.ipynb) |
| **Day 12 (Feb 14)**   | **Model Distillation**                             | Compressing models via knowledge distillation <br> [Notebook: Day_12/Model_Distillation.ipynb](Day_12/Model_Distillation.ipynb) |
| **Day 13 (Feb 15)**   | **Multimodal Models**                              | Exploring models like CLIP and DALL-E for image-text tasks <br> [Notebook: Day_13/Multimodal_Models.ipynb](Day_13/Multimodal_Models.ipynb) |
| **Day 14 (Feb 16)**   | **LLM Deployment Basics**                          | Deploying models using FastAPI/Flask, containerization <br> [Notebook: Day_14/LLM_Deployment.ipynb](Day_14/LLM_Deployment.ipynb) |
| **Day 15 (Feb 17)**   | **Monitoring LLMs in Production**                  | Tools & metrics: MLflow, drift detection, performance monitoring <br> [Notebook: Day_15/LLM_Monitoring.ipynb](Day_15/LLM_Monitoring.ipynb) |
| **Day 16 (Feb 18)**   | **Building a Domain-Specific Chatbot**             | Fine-tuning chatbots for specific domains <br> [Notebook: Day_16/Domain_Chatbot.ipynb](Day_16/Domain_Chatbot.ipynb) |
| **Day 17 (Feb 19)**   | **Advanced RAG Pipelines**                         | Hybrid search, query rewriting for robust Q&A <br> [Notebook: Day_17/Advanced_RAG.ipynb](Day_17/Advanced_RAG.ipynb) |
| **Day 18 (Feb 20)**   | **LLM Security**                                   | Defending against prompt injection & adversarial attacks <br> [Notebook: Day_18/LLM_Security.ipynb](Day_18/LLM_Security.ipynb) |
| **Day 19 (Feb 21)**   | **Low-Code LLM Tools**                             | Rapid prototyping with OpenAI API, ChatGPT Plugins, Hugging Face Spaces <br> [Notebook: Day_19/Low_Code_LLM.ipynb](Day_19/Low_Code_LLM.ipynb) |
| **Day 20 (Feb 22)**   | **LLM Explainability**                             | Using SHAP, LIME, and attention maps for transparency <br> [Notebook: Day_20/LLM_Explainability.ipynb](Day_20/LLM_Explainability.ipynb) |
| **Day 21 (Feb 23)**   | **Introduction to Document Summarization**         | Extractive vs. abstractive methods, use cases (e.g., summarizing articles) <br> [Folder: Day_21/Summarization](Day_21/Summarization) |
| **Day 22 (Feb 24)**   | **Fine-Tuning a Summarization Model**              | Adapting pre-trained models like BART/T5 for domain-specific tasks <br> [Folder: Day_22/Summarization](Day_22/Summarization) |
| **Day 23 (Feb 25)**   | **Building a Summarization API**                   | Deploying summarization models as REST APIs <br> [Folder: Day_23/Summarization](Day_23/Summarization) |
| **Day 24 (Feb 26)**   | **Introduction to Recommendation Systems**         | Overview of collaborative filtering vs. content-based approaches <br> [Folder: Day_24/Recommendation](Day_24/Recommendation) |
| **Day 25 (Feb 27)**   | **Fine-Tuning a Recommendation Model**             | Tailoring LLMs for personalized recommendations <br> [Folder: Day_25/Recommendation](Day_25/Recommendation) |
| **Day 26 (Feb 28)**   | **Building a Recommendation API**                  | Deploying recommendation models using FastAPI/Flask <br> [Folder: Day_26/Recommendation](Day_26/Recommendation) |
| **Day 27 (Mar 1)**    | **Introduction to Conversational Agents**          | Understanding memory in dialogue systems and use cases <br> [Folder: Day_27/Conversational_Agent](Day_27/Conversational_Agent) |
| **Day 28 (Mar 2)**    | **Integrating Memory into Conversational Agents**    | Adding short-term and long-term memory for multi-turn conversations <br> [Folder: Day_28/Conversational_Agent](Day_28/Conversational_Agent) |
| **Day 29 (Mar 3)**    | **Fine-Tuning a Conversational Agent**             | Customizing LLMs on dialogue datasets for domain-specific interactions <br> [Folder: Day_29/Conversational_Agent](Day_29/Conversational_Agent) |
| **Day 30 (Mar 4)**    | **Deploying a Conversational Agent**               | Making your conversational agent accessible via API endpoints <br> [Folder: Day_30/Conversational_Agent](Day_30/Conversational_Agent) |

*For detailed explanations, coding tasks, and outputs, click on the respective notebook or folder links!*

---

## üìÇ Repository Structure

```
30-Day-GenAI-LLM-Marathon/
‚îú‚îÄ‚îÄ Day_1/
‚îÇ   ‚îî‚îÄ‚îÄ Intro_to_Generative_AI.ipynb
‚îú‚îÄ‚îÄ Day_2/
‚îÇ   ‚îî‚îÄ‚îÄ Transformer_Implementation.ipynb
‚îú‚îÄ‚îÄ Day_3/
‚îÇ   ‚îî‚îÄ‚îÄ HuggingFace_Basics.ipynb
‚îú‚îÄ‚îÄ ... (more daily folders)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

Each day‚Äôs folder contains:
- **Jupyter Notebooks**: Detailed code, theory, and tasks.
- **README Files**: Summaries of concepts and learnings.
- **Datasets & Resources**: Provided or linked as needed.

---

## üèÜ How to Use This Repository

1. **Clone the Repository:**
   ```bash
   git clone https://github.com/your-username/30-Day-GenAI-LLM-Marathon.git
   cd 30-Day-GenAI-LLM-Marathon
   ```
2. **Install Dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
3. **Explore Daily Content:**
   - Navigate to any day‚Äôs folder.
   - Open the Jupyter Notebook using Jupyter or Google Colab.
   - Run and modify the code to deepen your learning.

---

## ü§ù Contributing & Collaboration

I welcome contributions and ideas!  
- **Fork the repository** and create a new branch.
- **Submit pull requests** for improvements, additional examples, or new projects.
- Join the conversation via **Issues** for feedback and discussions.

---

## üì¢ Why This Marathon?

I started this challenge to:
- Build a **solid foundation** in Generative AI and LLMs.
- Work on **real-world projects** that bridge theory with practical application.
- Share my learning process to **inspire and support** others on their AI journey.

If you enjoy this project, please **star ‚≠ê the repository** and share your feedback!

---

Thank you for exploring the **30-Day GenAI & LLM Marathon**!  
Happy Learning & Coding! üöÄ

---
