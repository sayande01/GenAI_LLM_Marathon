{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T15:58:30.541030Z","iopub.execute_input":"2025-02-12T15:58:30.541341Z","iopub.status.idle":"2025-02-12T15:58:31.502584Z","shell.execute_reply.started":"2025-02-12T15:58:30.541305Z","shell.execute_reply":"2025-02-12T15:58:31.501612Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install langchain_huggingface","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T15:59:51.672763Z","iopub.execute_input":"2025-02-12T15:59:51.673166Z","iopub.status.idle":"2025-02-12T15:59:56.024659Z","shell.execute_reply.started":"2025-02-12T15:59:51.673140Z","shell.execute_reply":"2025-02-12T15:59:56.023539Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain_huggingface\n  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.28.1)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.3.25)\nRequirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (3.3.1)\nRequirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (0.21.0)\nRequirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain_huggingface) (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\nRequirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.2.3)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.11.0a1)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.0.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (2024.11.6)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\nRequirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.28.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers>=4.39.0->langchain_huggingface) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.2.2)\nDownloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\nInstalling collected packages: langchain_huggingface\nSuccessfully installed langchain_huggingface-0.1.2\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint,HuggingFacePipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T16:10:45.595046Z","iopub.execute_input":"2025-02-12T16:10:45.595327Z","iopub.status.idle":"2025-02-12T16:10:45.599143Z","shell.execute_reply.started":"2025-02-12T16:10:45.595307Z","shell.execute_reply":"2025-02-12T16:10:45.598085Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Importing Modules from langchain_huggingface\n\nThe `langchain_huggingface` library provides integration between Hugging Face models and LangChain, enabling easy use of Hugging Face's NLP models in conversational AI, retrieval-augmented generation (RAG), and other applications.\n\n### Imported Classes:\n1. **ChatHuggingFace**  \n   - This class allows interaction with Hugging Face models in a conversational format.\n   - It is used for chat-based applications where an LLM responds to user input.\n\n2. **HuggingFaceEndpoint**  \n   - This enables interaction with Hugging Face's Inference API.\n   - Useful when deploying models hosted on Hugging Face’s cloud.\n\n3. **HuggingFacePipeline**  \n   - Integrates Hugging Face's `transformers` pipeline API with LangChain.\n   - This allows direct inference using locally downloaded or cloud-hosted models.\n\n### Usage:\n- If you want to use Hugging Face's hosted models, `HuggingFaceEndpoint` is useful.\n- If you prefer a locally running model, `HuggingFacePipeline` is the right choice.\n- `ChatHuggingFace` is ideal for chatbot-like applications.\n\nThese imports help in leveraging Hugging Face models within LangChain to build AI-driven applications.\n","metadata":{}},{"cell_type":"code","source":"llm = HuggingFaceEndpoint(\n    repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    task = \"text-generation\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T16:08:11.982330Z","iopub.execute_input":"2025-02-12T16:08:11.982616Z","iopub.status.idle":"2025-02-12T16:08:12.165504Z","shell.execute_reply.started":"2025-02-12T16:08:11.982594Z","shell.execute_reply":"2025-02-12T16:08:12.164700Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"llm = HuggingFacePipeline.from_model_id(\n    model_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n    task = 'text-generation',\n    pipeline_kwargs = dict(\n        temperature = 0.5,\n        max_new_tokens = 100\n    )\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T16:12:00.098292Z","iopub.execute_input":"2025-02-12T16:12:00.098590Z","iopub.status.idle":"2025-02-12T16:13:08.217931Z","shell.execute_reply.started":"2025-02-12T16:12:00.098570Z","shell.execute_reply":"2025-02-12T16:13:08.217235Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9769efe6ab34533bea5a85dcb5b4053"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9e109b83ce41b28e950e86ae3bb750"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caae5a0e62b146ecac12d68861a87b91"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"\n# Initializing a Language Model Using HuggingFacePipeline\n\n## Code Breakdown:\n```python\nllm = HuggingFacePipeline.from_model_id(\n    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n    task='text-generation',\n    pipeline_kwargs=dict(\n        temperature=0.5,\n        max_new_tokens=100\n    )\n)\n```\n\n## Explanation:\nThis code initializes a **language model (LLM)** using `HuggingFacePipeline` from `langchain_huggingface`. It loads the `TinyLlama-1.1B-Chat-v1.0` model from Hugging Face and sets it up for text generation.\n\n### **Parameters Used:**\n- **`model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0'`**  \n  - Specifies the model to be used.\n  - `TinyLlama-1.1B-Chat-v1.0` is a lightweight language model optimized for chatbot interactions.\n\n- **`task='text-generation'`**  \n  - Defines the task the model will perform.\n  - In this case, the model generates text based on the given input.\n\n- **`pipeline_kwargs`** (Optional Arguments for Model Behavior)\n  - **`temperature=0.5`**  \n    - Controls the randomness of the output.\n    - A lower value (e.g., 0.1) makes responses more deterministic, while a higher value (e.g., 1.0) increases randomness.\n  - **`max_new_tokens=100`**  \n    - Limits the maximum number of tokens the model can generate in response.\n\n### **What This Does:**\n- Loads the TinyLlama model.\n- Configures it for text generation.\n- Allows customization of output randomness and length.\n\n","metadata":{}},{"cell_type":"code","source":"model = ChatHuggingFace(llm = llm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T16:13:09.214945Z","iopub.execute_input":"2025-02-12T16:13:09.215544Z","iopub.status.idle":"2025-02-12T16:13:09.351967Z","shell.execute_reply.started":"2025-02-12T16:13:09.215516Z","shell.execute_reply":"2025-02-12T16:13:09.351097Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"\n## Code:\n```python\nmodel = ChatHuggingFace(llm=llm)\n```\n\n## Explanation:\nThis line of code creates a **chat-based language model** using `ChatHuggingFace` from `langchain_huggingface`. It wraps the previously initialized `llm` (which is based on `HuggingFacePipeline`) to enable conversational AI interactions.\n\n### **Components:**\n- **`ChatHuggingFace`**  \n  - A LangChain wrapper that facilitates interaction with chat models.\n  - It provides structured conversation handling, allowing better input-output management.\n\n- **`llm=llm`**  \n  - Passes the `llm` object (initialized using `HuggingFacePipeline`) to `ChatHuggingFace`, making it capable of handling conversational inputs.\n\n### **What This Does:**\n- Converts the Hugging Face LLM into a structured chat model.\n- Enables conversational AI interactions.\n- Can be integrated into LangChain pipelines for chatbot applications.\n\n### **Usage Example:**\nOnce `model` is initialized, it can be used to generate responses in a conversational manner:\n```python\nresponse = model.invoke(\"Hello, how are you?\")\nprint(response)\n```\nThis will output a response generated by the `TinyLlama-1.1B-Chat-v1.0` model.\n\n### **Key Benefits:**\n- Provides structured chat interactions.\n- Allows customization using Hugging Face models.\n- Can be easily integrated into LangChain-powered AI assistants.\n```","metadata":{}},{"cell_type":"code","source":"result = model.invoke(\"Give me a short passage on the City of joy Kolkata.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T16:14:56.931118Z","iopub.execute_input":"2025-02-12T16:14:56.931476Z","iopub.status.idle":"2025-02-12T16:14:59.256137Z","shell.execute_reply.started":"2025-02-12T16:14:56.931448Z","shell.execute_reply":"2025-02-12T16:14:59.255402Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(result.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T16:15:22.008472Z","iopub.execute_input":"2025-02-12T16:15:22.008815Z","iopub.status.idle":"2025-02-12T16:15:22.013124Z","shell.execute_reply.started":"2025-02-12T16:15:22.008761Z","shell.execute_reply":"2025-02-12T16:15:22.012301Z"}},"outputs":[{"name":"stdout","text":"<|user|>\nGive me a short passage on the City of joy Kolkata.</s>\n<|assistant|>\nKolkata, the City of Joy, is a city that has always been a source of pride for its people. It is a city that has been shaped by its history, culture, and traditions, and it is a city that continues to inspire and uplift its people.\n\nKolkata is a city that has been shaped by its history. It was founded in 1690 by the British East India Company, and it was named after the city\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}