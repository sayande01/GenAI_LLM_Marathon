{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics for LLMs\n",
        "\n",
        "---\n",
        "\n",
        "## Notebook Structure\n",
        "\n",
        "1. Metadata\n",
        "2. Title and Objective\n",
        "3. Model & Dataset Overview\n",
        "4. Concept Explanation\n",
        "   - What are Evaluation Metrics for LLMs?\n",
        "   - Why & Where do we use them?\n",
        "   - Overview of BLEU, ROUGE, Perplexity, and Human Evaluation\n",
        "5. Mathematical Intuition & Formulas\n",
        "   - BLEU\n",
        "   - ROUGE\n",
        "   - Perplexity\n",
        "6. Implementation and Examples\n",
        "   - Sample Candidate and Reference Texts\n",
        "   - Evaluating using BLEU, ROUGE, and Perplexity\n",
        "   - Comparing the Fine-Tuned Model vs. the Base Model\n",
        "7. Analysis of Results and Discussion on Limitations\n",
        "8. Conclusion and Learnings\n",
        "9. Acknowledgements\n"
      ],
      "metadata": {
        "id": "s5EskWTvQiz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Metadata\n",
        "\n",
        "- **Topic**: Evaluation Metrics for Large Language Models (LLMs)\n",
        "- **Metrics Covered**: BLEU, ROUGE, Perplexity\n",
        "- **Additional Discussion**: Human Evaluation vs. Automated Metrics, Limitations of Metrics\n",
        "- **Models Evaluated**: A fine-tuned model and a base (pre-trained) model\n",
        "- **Tech Stack**: Python, NLTK, Hugging Face Transformers (if applicable), Pandas, Matplotlib\n"
      ],
      "metadata": {
        "id": "XLaCK1IZQmG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Title and Objective\n",
        "\n",
        "### Title\n",
        "Evaluation Metrics for LLMs: Measuring Performance with BLEU, ROUGE, and Perplexity\n",
        "\n",
        "### Objective\n",
        "- **What?** Assess the performance of LLMs using automated evaluation metrics.\n",
        "- **Why?** To ensure models meet quality standards and to compare a fine-tuned model against a base model.\n",
        "- **How?** By computing BLEU, ROUGE, and Perplexity scores on sample outputs.\n",
        "- **Where?** These metrics are widely used in machine translation, summarization, and language modeling tasks.\n"
      ],
      "metadata": {
        "id": "YshIeZgVQnjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model & Dataset Overview\n",
        "\n",
        "In this notebook, we assume:\n",
        "- We have a **base model** (e.g., a pre-trained language model).\n",
        "- We have a **fine-tuned model** (e.g., further trained on a specific dataset).\n",
        "- For demonstration, we use a set of sample candidate texts generated by each model and corresponding reference texts (ground truth).\n",
        "\n",
        "*Note:* In a production setting, you would evaluate on a larger test set of model outputs.\n"
      ],
      "metadata": {
        "id": "dr8kpufUQo7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Concept Explanation\n",
        "\n",
        "### What are Evaluation Metrics for LLMs?\n",
        "Evaluation metrics are quantitative measures used to assess the performance and quality of language model outputs. They help us determine how well a model generates text compared to a reference (ground truth).\n",
        "\n",
        "### Why & Where Do We Use Them?\n",
        "- **Why?**  \n",
        "  - To compare different models or model versions.\n",
        "  - To guide model improvements and fine-tuning.\n",
        "- **Where?**  \n",
        "  - In machine translation (BLEU),\n",
        "  - In summarization tasks (ROUGE),\n",
        "  - In language modeling (Perplexity).\n",
        "\n",
        "### Overview of the Metrics\n",
        "- **BLEU (Bilingual Evaluation Understudy)**:  \n",
        "  Compares the overlap between n-grams of candidate and reference texts.\n",
        "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**:  \n",
        "  Focuses on recall by comparing overlapping units such as n-grams or word sequences.\n",
        "- **Perplexity**:  \n",
        "  Measures how well a probability model predicts a sample; lower perplexity indicates a better model.\n",
        "\n",
        "### Human Evaluation vs. Automated Metrics\n",
        "- **Human Evaluation**:  \n",
        "  Involves subjective assessments of fluency, coherence, and relevance.\n",
        "- **Automated Metrics**:  \n",
        "  Provide quick, reproducible measurements but may miss nuances that human evaluators capture.\n"
      ],
      "metadata": {
        "id": "jCXDT3J9QqWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Mathematical Intuition & Formulas\n",
        "\n",
        "### BLEU\n",
        "A simplified explanation:\n",
        "- **Formula**:  \n",
        "  `BLEU = BP * exp( sum( w_n * log(p_n) ) )`\n",
        "  - **BP**: Brevity penalty (penalizes overly short candidate texts)\n",
        "  - **p_n**: Precision for n-grams (overlap ratio)\n",
        "  - **w_n**: Weight for each n-gram level (typically equal weights)\n",
        "\n",
        "*Plain terms:*  \n",
        "It computes the ratio of matching n-grams between candidate and reference and applies a penalty if the candidate is too short.\n",
        "\n",
        "### ROUGE\n",
        "A simplified explanation:\n",
        "- **Commonly used ROUGE-N**:  \n",
        "  `ROUGE-N = (Number of matching n-grams) / (Total n-grams in the reference)`\n",
        "  \n",
        "*Plain terms:*  \n",
        "It measures the recall (how much of the reference is covered) by comparing overlapping n-grams.\n",
        "\n",
        "### Perplexity\n",
        "- **Formula**:  \n",
        "  `Perplexity = exp( - (1/N) * sum( log(P(word_i)) ) )`\n",
        "  - **N**: Number of words\n",
        "  - **P(word_i)**: The probability assigned by the model to the i-th word\n",
        "\n",
        "*Plain terms:*  \n",
        "Lower perplexity means the model is better at predicting the next word; it’s the exponentiation of the negative average log-probability.\n"
      ],
      "metadata": {
        "id": "fjb0fi40QsS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Implementation and Examples"
      ],
      "metadata": {
        "id": "kNrJhcYuQwH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# For demonstration, we create sample candidate and reference texts.\n",
        "# In practice, these would come from your model outputs.\n",
        "\n",
        "# Sample texts for the base model and fine-tuned model\n",
        "reference_texts = [\n",
        "    \"The weather is nice today and perfect for a walk in the park.\",\n",
        "    \"Machine learning techniques are revolutionizing the field of artificial intelligence.\"\n",
        "]\n",
        "\n",
        "candidate_texts_base = [\n",
        "    \"The weather is good today and ideal for a stroll in the park.\",\n",
        "    \"Machine learning methods are changing the field of AI.\"\n",
        "]\n",
        "\n",
        "candidate_texts_finetuned = [\n",
        "    \"Today the weather is pleasant and it is a perfect day for a park walk.\",\n",
        "    \"Techniques in machine learning are transforming the realm of artificial intelligence.\"\n",
        "]\n",
        "\n",
        "# Display sample texts\n",
        "print(\"Reference Texts:\")\n",
        "for text in reference_texts:\n",
        "    print(\"-\", text)\n",
        "\n",
        "print(\"\\nCandidate Texts (Base Model):\")\n",
        "for text in candidate_texts_base:\n",
        "    print(\"-\", text)\n",
        "\n",
        "print(\"\\nCandidate Texts (Fine-Tuned Model):\")\n",
        "for text in candidate_texts_finetuned:\n",
        "    print(\"-\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK2IgpF-Qkzb",
        "outputId": "b0887375-18eb-4dc1-c91f-71cb6ac1f4ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Texts:\n",
            "- The weather is nice today and perfect for a walk in the park.\n",
            "- Machine learning techniques are revolutionizing the field of artificial intelligence.\n",
            "\n",
            "Candidate Texts (Base Model):\n",
            "- The weather is good today and ideal for a stroll in the park.\n",
            "- Machine learning methods are changing the field of AI.\n",
            "\n",
            "Candidate Texts (Fine-Tuned Model):\n",
            "- Today the weather is pleasant and it is a perfect day for a park walk.\n",
            "- Techniques in machine learning are transforming the realm of artificial intelligence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of Sample Texts\n",
        "\n",
        "- **Reference Texts:** These are the ground-truth sentences we expect the models to generate.\n",
        "- **Candidate Texts (Base Model):** Output from the base model.  \n",
        "- **Candidate Texts (Fine-Tuned Model):** Output from the fine-tuned model.\n",
        "\n",
        "Reviewing these texts gives us context before calculating the evaluation scores. Differences in wording, synonym usage, or structure will influence the BLEU, ROUGE, and Perplexity scores.\n"
      ],
      "metadata": {
        "id": "km3dXSCURg1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Evaluating BLEU Score using NLTK\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Ensure that NLTK data is available (you might need to run nltk.download('punkt'))\n",
        "smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "def compute_bleu(candidate, reference):\n",
        "    # Tokenize the sentences\n",
        "    candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
        "    reference_tokens = [nltk.word_tokenize(reference.lower())]\n",
        "    return sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smooth_fn)\n",
        "\n",
        "# Evaluate BLEU scores for base and fine-tuned models\n",
        "bleu_scores_base = [compute_bleu(c, r) for c, r in zip(candidate_texts_base, reference_texts)]\n",
        "bleu_scores_finetuned = [compute_bleu(c, r) for c, r in zip(candidate_texts_finetuned, reference_texts)]\n",
        "\n",
        "print(\"BLEU Scores (Base Model):\", bleu_scores_base)\n",
        "print(\"BLEU Scores (Fine-Tuned Model):\", bleu_scores_finetuned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK9aOkWcQxRv",
        "outputId": "ebce5eaa-a138-4ca2-c318-d80e828d021d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Scores (Base Model): [0.31314224813827346, 0.12927595103001124]\n",
            "BLEU Scores (Fine-Tuned Model): [0.0932304601086144, 0.2790159393585827]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU Score Explanation\n",
        "\n",
        "- **What the Code Does:**  \n",
        "  - The `compute_bleu` function tokenizes the candidate and reference sentences, then computes the BLEU score using NLTK’s `sentence_bleu`.\n",
        "  - We calculate BLEU scores for each pair of candidate and reference texts for both models.\n",
        "  \n",
        "- **Interpreting the Scores:**  \n",
        "  - **Higher BLEU Score:** Indicates a greater overlap of n‑grams (words, bi-grams, etc.) between candidate and reference, suggesting closer similarity to the ground truth.\n",
        "  - Compare the BLEU scores of the base and fine-tuned models to assess which one produces outputs closer to the reference texts.\n",
        "\n",
        "### Interpretation of BLEU Scores\n",
        "\n",
        "- **Base Model BLEU Scores:** [0.3131, 0.1293]  \n",
        "- **Fine-Tuned Model BLEU Scores:** [0.0932, 0.2790]\n",
        "\n",
        "**What this means:**  \n",
        "BLEU measures the overlap of n‑grams between the candidate and reference texts. For the first sample, the base model achieves a higher BLEU score (0.3131) compared to the fine-tuned model (0.0932), indicating that its output has more n‑gram similarity with the reference. Conversely, for the second sample, the fine-tuned model (0.2790) outperforms the base model (0.1293), suggesting better overlap with the reference text.  \n",
        "\n",
        "**Overall:**  \n",
        "The mixed BLEU scores across samples imply that while one model might perform better on a specific instance, a larger evaluation set is needed to conclusively determine which model consistently produces outputs closer to the references.\n"
      ],
      "metadata": {
        "id": "bfwaAvBlRkgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imuGouMGRK31",
        "outputId": "829c71c1-2781-4cff-af1d-8d5f3d800346"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=507ac6bb7213cb2ba8f9fd27118224e416ea955226ee9d4bf6695fd0f309fdaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2 Evaluating ROUGE Score using the 'rouge' package\n",
        "# You might need to install the package: pip install rouge-score\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def compute_rouge(candidate, reference):\n",
        "    scores = scorer.score(reference, candidate)\n",
        "    # For demonstration, we return the F-measure of ROUGE-1 and ROUGE-L\n",
        "    rouge1 = scores['rouge1'].fmeasure\n",
        "    rougeL = scores['rougeL'].fmeasure\n",
        "    return (rouge1 + rougeL) / 2\n",
        "\n",
        "# Evaluate ROUGE scores for base and fine-tuned models\n",
        "rouge_scores_base = [compute_rouge(c, r) for c, r in zip(candidate_texts_base, reference_texts)]\n",
        "rouge_scores_finetuned = [compute_rouge(c, r) for c, r in zip(candidate_texts_finetuned, reference_texts)]\n",
        "\n",
        "print(\"ROUGE Scores (Base Model):\", rouge_scores_base)\n",
        "print(\"ROUGE Scores (Fine-Tuned Model):\", rouge_scores_finetuned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuBuDgWBQ2Hy",
        "outputId": "54e392cf-2aed-4782-8413-27957f9593c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores (Base Model): [0.7692307692307693, 0.631578947368421]\n",
            "ROUGE Scores (Fine-Tuned Model): [0.6428571428571428, 0.7142857142857143]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROUGE Score Explanation\n",
        "\n",
        "- **What the Code Does:**  \n",
        "  - The code uses the `rouge_scorer` from the `rouge-score` package to compute ROUGE-1 and ROUGE-L scores for each candidate-reference pair.\n",
        "  - It then averages the F-measures of both ROUGE-1 and ROUGE-L to obtain a single score per sentence.\n",
        "  \n",
        "- **Interpreting the Scores:**  \n",
        "  - **Higher ROUGE Score:** Suggests a better recall of n‑gram overlap, meaning the candidate text covers more of the key content in the reference.\n",
        "  - Use these scores to compare how well each model’s outputs capture the essence of the reference texts.\n",
        "\n",
        "### Interpretation of ROUGE Scores\n",
        "\n",
        "- **Base Model ROUGE Scores:** [0.7692, 0.6316]  \n",
        "- **Fine-Tuned Model ROUGE Scores:** [0.6429, 0.7143]\n",
        "\n",
        "**What this means:**  \n",
        "ROUGE focuses on recall—measuring the fraction of key n‑grams or phrases in the reference text that appear in the candidate text. For sample 1, the base model's ROUGE score (0.7692) is higher than that of the fine-tuned model (0.6429), suggesting that the base model’s output covers more of the important content. For sample 2, the fine-tuned model scores higher (0.7143) compared to the base model (0.6316), indicating better coverage of the reference content for that sample.\n",
        "\n",
        "**Overall:**  \n",
        "These results demonstrate that performance can vary by sample, and the choice between models might depend on the specific content or context. Averaging results over a larger test set is recommended for a robust evaluation.\n"
      ],
      "metadata": {
        "id": "rKrGu5jnRmuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.3 Evaluating Perplexity\n",
        "# For perplexity, assume we have a language model that can score text.\n",
        "# We use a dummy function for demonstration. In practice, use your LM's API.\n",
        "\n",
        "import math\n",
        "\n",
        "def compute_perplexity(text, model_log_prob_func):\n",
        "    \"\"\"\n",
        "    Compute perplexity given a text and a function that returns log probability for each token.\n",
        "    model_log_prob_func should accept a list of tokens and return the sum of log probabilities.\n",
        "    \"\"\"\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    N = len(tokens)\n",
        "    if N == 0:\n",
        "        return float('inf')\n",
        "    log_prob_sum = model_log_prob_func(tokens)\n",
        "    avg_log_prob = log_prob_sum / N\n",
        "    perplexity = math.exp(-avg_log_prob)\n",
        "    return perplexity\n",
        "\n",
        "# Dummy log probability function: for demonstration, we assume a fixed probability for each token.\n",
        "def dummy_log_prob(tokens):\n",
        "    # Let's assume each token has a probability of 0.1 (log(0.1) = -2.3026)\n",
        "    return -2.3026 * len(tokens)\n",
        "\n",
        "# Evaluate perplexity for candidate texts\n",
        "perplexity_base = [compute_perplexity(c, dummy_log_prob) for c in candidate_texts_base]\n",
        "perplexity_finetuned = [compute_perplexity(c, dummy_log_prob) for c in candidate_texts_finetuned]\n",
        "\n",
        "print(\"Perplexity (Base Model):\", perplexity_base)\n",
        "print(\"Perplexity (Fine-Tuned Model):\", perplexity_finetuned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okTbXx1TRDut",
        "outputId": "42e79576-e2c7-491e-9c6c-f06c007c6348"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity (Base Model): [10.000149071170647, 10.000149071170643]\n",
            "Perplexity (Fine-Tuned Model): [10.000149071170643, 10.000149071170643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perplexity Explanation\n",
        "\n",
        "- **What the Code Does:**  \n",
        "  - The `compute_perplexity` function tokenizes the text and uses a provided log probability function to compute the average log probability.\n",
        "  - We then exponentiate the negative average to obtain the perplexity.\n",
        "  - Here, we use a `dummy_log_prob` function for demonstration that assumes a constant probability for each token.\n",
        "  \n",
        "- **Interpreting the Scores:**  \n",
        "  - **Lower Perplexity:** Indicates that the model is more confident in its predictions (i.e., the candidate text is more “predictable” by the model).\n",
        "  - Compare the perplexity values between the base and fine-tuned models to see which one is better at predicting the text.\n",
        "\n",
        "### Interpretation of Perplexity Scores\n",
        "\n",
        "- **Base Model Perplexity Scores:** [10.00015, 10.00015]  \n",
        "- **Fine-Tuned Model Perplexity Scores:** [10.00015, 10.00015]\n",
        "\n",
        "**What this means:**  \n",
        "Perplexity quantifies how well a model predicts the next word in a sequence—a lower perplexity indicates better predictive performance. In this demonstration, both models yield virtually identical perplexity values (approximately 10), meaning that, under the dummy probability function used here, they are equally “confident” in predicting the text.\n",
        "\n",
        "**Overall:**  \n",
        "Since we used a dummy log probability function for demonstration purposes, the perplexity scores do not differentiate the models. In a real-world scenario with a proper language model providing log probabilities, differences in perplexity might better reflect the models' predictive capabilities.\n"
      ],
      "metadata": {
        "id": "Getop8fwRo9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Analysis of Results and Discussion on Limitations\n",
        "\n",
        "- **BLEU**:  \n",
        "  Measures n-gram precision. While higher scores indicate more overlap, BLEU may not capture semantic meaning fully.\n",
        "  \n",
        "- **ROUGE**:  \n",
        "  Focuses on recall and captures overlapping phrases. It is useful in summarization tasks but may penalize valid paraphrases.\n",
        "  \n",
        "- **Perplexity**:  \n",
        "  Indicates how well a language model predicts a text. Lower perplexity is better; however, it is sensitive to the underlying probability estimates.\n",
        "  \n",
        "- **Human vs. Automated Evaluation**:  \n",
        "  Automated metrics provide quick, reproducible results but can miss nuanced language qualities that human evaluators would notice.\n",
        "\n",
        "- **Limitations**:  \n",
        "  - **BLEU/ROUGE**: Can be insensitive to meaning and may reward word overlap without semantic correctness.\n",
        "  - **Perplexity**: Depends on the quality of the language model; not always correlated with human judgments.\n"
      ],
      "metadata": {
        "id": "cFYqHrH5RQkj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xPOv0GeROpx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}